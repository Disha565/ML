{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74586,"databundleVersionId":8130765,"sourceType":"competition"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom os.path import join\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation\nimport torch.optim as optim\n\n# Define the CNN model\nclass AgePredictionModel(nn.Module):\n    def __init__(self):\n        super(AgePredictionModel, self).__init__()\n        self.features = torchvision.models.resnet18(pretrained=True)\n        self.features.fc = nn.Linear(512, 1)  # Replace the final fully connected layer for age prediction\n\n    def forward(self, x):\n        x = self.features(x)\n        return x.view(x.size(0))\n\n# Define dataset class\nclass AgeDataset(torch.utils.data.Dataset):\n    def __init__(self, data_path, annot_path, train=True, transform=None):\n        super(AgeDataset, self).__init__()\n        self.annot_path = annot_path\n        self.data_path = data_path\n        self.train = train\n        self.ann = pd.read_csv(annot_path)\n        self.files = self.ann['file_id']\n        if train:\n            self.ages = self.ann['age']\n        self.transform = transform\n\n    @staticmethod    \n    def _convert_image_to_rgb(image):\n        return image.convert(\"RGB\")\n\n    def read_img(self, file_name):\n        im_path = join(self.data_path,file_name)   \n        img = Image.open(im_path)\n        img = self._convert_image_to_rgb(img)\n        return img\n\n    def __getitem__(self, index):\n        file_name = self.files[index]\n        img = self.read_img(file_name)\n        if self.transform:\n            img = self.transform(img)\n        if self.train:\n            age = self.ages[index]\n            return img, age\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.files)\n\n# Define function for training the model\ndef train_model(train_loader, model, criterion, optimizer, num_epochs=25):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in tqdm(train_loader):\n            inputs = inputs.to(device)\n            labels = labels.float().to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n\n# Load data\ntrain_path = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/train'\ntrain_ann = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/train.csv'\ntest_path = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/test'\ntest_ann = '/kaggle/input/smai-24-age-prediction/content/faces_dataset/submission.csv'\n\n# Define transformations for training and testing data\ntrain_transform = Compose([\n    Resize(256),\n    RandomHorizontalFlip(),\n    RandomRotation(10),\n    CenterCrop(224),\n    ToTensor(),\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_transform = Compose([\n    Resize(224),\n    CenterCrop(224),\n    ToTensor(),\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create datasets and loaders\ntrain_dataset = AgeDataset(train_path, train_ann, train=True, transform=train_transform)\ntest_dataset = AgeDataset(test_path, test_ann, train=False, transform=test_transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Initialize the model, loss function, and optimizer\nmodel = AgePredictionModel()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\ntrain_model(train_loader, model, criterion, optimizer, num_epochs=25)\n\n# Function for making predictions\n@torch.no_grad()\ndef predict(loader, model):\n    model.eval()\n    predictions = []\n\n    for img in tqdm(loader):\n        img = img.to(device)\n        pred = model(img)\n        predictions.extend(pred.flatten().cpu().detach().tolist())\n\n    return predictions\n\n# Generate predictions\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\npreds = predict(test_loader, model)\n\n# Save predictions to submission CSV file\nsubmit = pd.read_csv('/kaggle/input/smai-24-age-prediction/content/faces_dataset/submission.csv')\nsubmit['age'] = preds\nsubmit.to_csv('submissions.csv', index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-17T09:40:48.137981Z","iopub.execute_input":"2024-04-17T09:40:48.138317Z","iopub.status.idle":"2024-04-17T10:27:50.670788Z","shell.execute_reply.started":"2024-04-17T09:40:48.138290Z","shell.execute_reply":"2024-04-17T10:27:50.669881Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n100%|██████████| 334/334 [01:53<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25, Loss: 163.9712\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:52<00:00,  2.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/25, Loss: 69.9095\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:56<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/25, Loss: 62.8266\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:54<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/25, Loss: 56.7412\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/25, Loss: 53.3799\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/25, Loss: 51.2282\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/25, Loss: 47.6512\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/25, Loss: 45.4417\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/25, Loss: 43.2646\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/25, Loss: 41.2698\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:53<00:00,  2.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/25, Loss: 40.3755\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:53<00:00,  2.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/25, Loss: 36.9715\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/25, Loss: 35.8230\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:52<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/25, Loss: 33.1554\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/25, Loss: 32.3381\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:52<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/25, Loss: 30.6750\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:52<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/25, Loss: 29.4488\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:52<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/25, Loss: 27.2747\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:55<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/25, Loss: 26.0457\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:53<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/25, Loss: 25.0704\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/25, Loss: 22.3764\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:53<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/25, Loss: 21.8617\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/25, Loss: 20.5774\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:51<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/25, Loss: 19.9180\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 334/334 [01:52<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/25, Loss: 19.3136\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 31/31 [00:08<00:00,  3.59it/s]\n","output_type":"stream"}]}]}